{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm2 import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pickle\n",
    "from random import *\n",
    "import theano.tensor as T\n",
    "def _p(pp, name):\n",
    "    return '%s_%s' % (pp, name)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Xavier Initialisation </br>\n",
    "<br>The Bias of the forget gates <b>b[n:n+n])</b> are set to 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getlayerx(d,pref,n,nin):\n",
    "    mu=0.0\n",
    "    sigma=0.1\n",
    "    U = np.random.normal(mu,sigma, (4*n,n))/np.sqrt(nin)\n",
    "    U=np.array(U,dtype=np.float32)\n",
    "    W =np.random.normal(mu,sigma, (4*n,nin))/np.sqrt(nin)\n",
    "    W=np.array(W,dtype=np.float32)\n",
    "    \n",
    "    d[_p(pref, 'U')] = U\n",
    "    #b = numpy.zeros((n * 300,))+1.5\n",
    "    b = np.random.uniform(-0.5,0.5,size=(4*n,))\n",
    "    b[n:n*2]=1.5 \n",
    "    d[_p(pref, 'W')] = W\n",
    "    d[_p(pref, 'b')] = b.astype(config.floatX)\n",
    "    return d\n",
    "def creatrnnx():\n",
    "    newp=OrderedDict()\n",
    "    print (\"Creating neural network\")\n",
    "    newp=getlayerx(newp,'1lstm1',50,300)\n",
    "  \n",
    "    newp=getlayerx(newp,'2lstm1',50,300)\n",
    "    \n",
    "    return newp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim word2vec has 300 dimension vectors\n",
    "<br> <b>Note: </b> We don't train the embedding vectors anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix='lstm'\n",
    "dim_proj=300 \n",
    "noise_std=0.\n",
    "options=locals().copy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newp=creatrnnx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the weights of both LSTM's equal LSTM_A and LSTM_B\n",
    "<br> Prefix '1' is used to represent LSTM_A\n",
    "<br> Prefix '2' for LSTM_B\n",
    "<br>Suffix \"_U\" for candidature\n",
    "<br> \"_b\" for Bias gates\n",
    "<br> \"_W\" for input gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in newp.keys():\n",
    "    if i[0]=='1':\n",
    "        newp[i]=newp['2'+i[1:]]\n",
    "        print '2'+i[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tensor.vector('y', dtype='float32')\n",
    "mask11 = tensor.matrix('mask11', dtype=config.floatX)\n",
    "mask21 = tensor.matrix('mask21', dtype=config.floatX)\n",
    "emb11=theano.tensor.ftensor3('emb11')\n",
    "emb21=theano.tensor.ftensor3('emb21')\n",
    "\n",
    "#newp=pickle.load(open(\"33814200semevalorig.p\",\"rb\"))\n",
    "\n",
    "tnewp=init_tparams(newp)\n",
    "trng = RandomStreams(1234)\n",
    "use_noise = theano.shared(numpy_floatX(0.))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Layer\n",
    "<br> nhd=number of hidden units\n",
    "<br>\"prefix\"-> Name of the LSTM (Stored in the form of a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_layer2(tparams, state_below, options, prefix='lstm', mask=None,nhd=None):\n",
    "    nsteps = state_below.shape[0]\n",
    "    if state_below.ndim == 3:\n",
    "        n_samples = state_below.shape[1]\n",
    "    else:\n",
    "        n_samples = 1\n",
    "\n",
    "    assert mask is not None\n",
    "\n",
    "    def _slice(_x, n, dim):\n",
    "        if _x.ndim == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        return _x[:, n * dim:(n + 1) * dim]\n",
    "\n",
    "    def _step(m_, x_, h_, c_):\n",
    "        preact = tensor.dot(h_, tparams[_p(prefix, 'U')].T)\n",
    "        preact += x_\n",
    "        preact += tparams[_p(prefix, 'b')]\n",
    "\n",
    "        i = tensor.nnet.sigmoid(_slice(preact, 0, nhd))\n",
    "        f = tensor.nnet.sigmoid(_slice(preact, 1, nhd))\n",
    "        o = tensor.nnet.sigmoid(_slice(preact, 2, nhd))\n",
    "        c = tensor.tanh(_slice(preact, 3, nhd))\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n",
    "\n",
    "        h = o * tensor.tanh(c)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return [h, c]\n",
    "\n",
    "    state_below = (tensor.dot(state_below, tparams[_p(prefix, 'W')].T) +\n",
    "                   tparams[_p(prefix, 'b')])\n",
    "    #print \"hvals\"\n",
    "    dim_proj = nhd\n",
    "    [hvals,yvals], updates = theano.scan(_step,\n",
    "                                sequences=[mask, state_below],\n",
    "                                outputs_info=[tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj),\n",
    "                                              tensor.alloc(numpy_floatX(0.),\n",
    "                                                           n_samples,\n",
    "                                                           dim_proj)],\n",
    "                                name=_p(prefix, '_layers'),\n",
    "                                n_steps=nsteps)\n",
    "    return hvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(state_before, use_noise, rrng,rate):\n",
    "    proj = tensor.switch(use_noise,\n",
    "                         (state_before *rrng),\n",
    "                         state_before * (1-rate))\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified function to add a layer\n",
    "<br>To enable Dropout (optional) set used=<font color=\"green\">True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getpl(prevlayer,pre,mymask,used,rrng,size):\n",
    "    proj = lstm_layer(tnewp, prevlayer, options,\n",
    "                                        prefix=pre,\n",
    "                                        mask=mymask,nhd=size)\n",
    "    #proj = (proj * mymask[:, :, None]).sum(axis=0)\n",
    "    #proj = proj / mymask.sum(axis=0)[:, None]\n",
    "    if used:\n",
    "        print \"Added dropout\"\n",
    "        proj = dropout_layer(proj, use_noise, rrng,0.5)\n",
    "        \n",
    "    return proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rrng->Used when Dropout is enabled\n",
    "<br> <b>proj11</b> takes the inputs as embedding matrix <b>emb1</b> and gives the o/p of the LSTM_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rate=0.5\n",
    "rrng=trng.binomial((mask11.shape[0],mask11.shape[1],50),p=1-rate, n=1,dtype=emb11.dtype)\n",
    "\n",
    "proj11=getpl2(emb11,'1lstm1',mask11,False,rrng,50)[-1]\n",
    "\n",
    "\n",
    "proj21=getpl2(emb21,'2lstm1',mask21,False,rrng,50)[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity= exp(-L1_norm(difference of outputs))\n",
    "$$sim = e^{(-|proj11-proj21|)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input is in the format:\n",
    "<br>\n",
    "(Max no. of words in batch, No. of Samples, 300)\n",
    "<br> The output proj11[-1] will have dimensions( No. of samples, Hidden unit dimension), so (No. of samples,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# norm similarity  symmetric\n",
    "\n",
    "batchsize=64\n",
    "bts=float(batchsize)\n",
    "\n",
    "dif=(proj21-proj11).norm(L=1,axis=1)\n",
    "\n",
    "\n",
    "#dropout diff\n",
    "#dif=(proj11-proj21).norm(L=1,axis=0)\n",
    "\n",
    "\n",
    "#si=T.diagonal(T.dot(proj12,proj22.T))/T.nnet.softmax(dif)\n",
    "#sim=T.reshape(si,(si.shape[1],))\n",
    "s2=T.exp(-dif)\n",
    "sim=T.clip(s2,1e-7,1.0-1e-7)\n",
    "lr = tensor.scalar(name='lr')\n",
    "ys=T.clip((y-1.0)/4.0,1e-7,1.0-1e-7)\n",
    "cost=T.mean((sim - ys) ** 2)\n",
    "ns=emb11.shape[1]\n",
    "f2sim=theano.function([emb11,mask11,emb21,mask21],sim,allow_input_downcast=True)\n",
    "f_cost=theano.function([emb11,mask11,emb21,mask21,y],cost,allow_input_downcast=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradi refers to gradients wrt. cost, and is a list containing gradients to be update weights\n",
    "<br> We average out the gradients by appending to another list grads[]\n",
    "<br> So, we average out the gradients : wrt LSTM_A and wrt LSTM_B\n",
    "<br> i.e, gradient= (grad(wrt(LSTM_A)+grad(wrt(LSTM_B))/2.0 to maintain the symmetricity between either LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradi = tensor.grad(cost, wrt=tnewp.values())#/bts\n",
    "#newp[i]=newp['2'+i[1:]]\n",
    "grads=[]\n",
    "l=len(gradi)\n",
    "for i in range(0,l/2):\n",
    "    gravg=(gradi[i]+gradi[i+l/2])/(2.0)\n",
    "    #print i,i+9\n",
    "    grads.append(gravg)\n",
    "for i in range(0,len(tnewp.keys())/2):\n",
    "    grads.append(grads[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisers\n",
    "<br> Recommended: Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adadelta(lr, tparams, grads, emb11,mask11,emb21,mask21,y, cost):\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                 name='%s_rup2' % k)\n",
    "                   for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, (0.95 * rg2 + 0.05* (g ** 2)))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([emb11,mask11,emb21,mask21,y], cost, updates=zgup + rg2up,allow_input_downcast=True,\n",
    "                                    name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-tensor.sqrt(ru2 + 1e-6) / tensor.sqrt(rg2 + 1e-6) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    ru2up = [(ru2, (0.95 * ru2 + 0.05 * (ud ** 2)))\n",
    "             for ru2, ud in zip(running_up2,updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([lr], [], updates=ru2up + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "def sgd(lr, tparams, grads, emb11,mask11,emb21,mask21,y, cost):\n",
    "    \n",
    "    gshared = [theano.shared(p.get_value() * 0., name='%s_grad' % k)\n",
    "               for k, p in tparams.iteritems()]\n",
    "    gsup = [(gs, g) for gs, g in zip(gshared, grads)]\n",
    "    f_grad_shared = theano.function([emb11,mask11,emb21,mask21,y], cost, updates=gsup,\n",
    "                                    name='sgd_f_grad_shared')\n",
    "    pup = [(p, p - lr * g) for p, g in zip(tparams.values(), gshared)]\n",
    "    f_update = theano.function([lr], [], updates=pup,\n",
    "                               name='sgd_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "\n",
    "def rmsprop(lr, tparams, grads, emb11,mask11,emb21,mask21,y, cost):\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.iteritems()]\n",
    "    running_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                   name='%s_rgrad' % k)\n",
    "                     for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rgup = [(rg, 0.95 * rg + 0.05 * g) for rg, g in zip(running_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([emb11,mask11,emb21,mask21,y], cost,\n",
    "                                    updates=zgup + rgup + rg2up,\n",
    "                                    name='rmsprop_f_grad_shared')\n",
    "\n",
    "    updir = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                           name='%s_updir' % k)\n",
    "             for k, p in tparams.iteritems()]\n",
    "    updir_new = [(ud, 0.9 * ud - 1e-4 * zg / tensor.sqrt(rg2 - rg ** 2 + 1e-4))\n",
    "                 for ud, zg, rg, rg2 in zip(updir, zipped_grads, running_grads,\n",
    "                                            running_grads2)]\n",
    "    param_up = [(p, p + udn[1])\n",
    "                for p, udn in zip(tparams.values(), updir_new)]\n",
    "    f_update = theano.function([lr], [], updates=updir_new + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='rmsprop_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes a long time to compute\n",
    "f_grad_shared, f_update = adadelta(lr, tnewp, grads,\n",
    "                                    emb11,mask11,emb21,mask21,y, cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
